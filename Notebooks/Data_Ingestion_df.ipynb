{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import pandas as pd\n",
    "import hsml\n",
    "from constants import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from itertools import combinations\n",
    "\n",
    "file_path = 'C:/Users/rr010/OneDrive/Desktop/Class/Projects/TruckDelay_Classification/Data/Training_Data/traffic_table.csv'\n",
    "\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "def check_unique_columns(data):\n",
    "    unique_columns = []\n",
    "    for col in data.columns:\n",
    "        if data[col].is_unique and data[col].notna().all():\n",
    "            unique_columns.append(col)\n",
    "    return unique_columns\n",
    "\n",
    "def check_composite_keys(data, max_columns=3):\n",
    "    composite_keys = []\n",
    "    for r in range(2, max_columns+1):\n",
    "        for cols in combinations(data.columns, r):\n",
    "            print(cols)\n",
    "            subset = data[list(cols)].dropna()\n",
    "            print(subset.duplicated().sum())\n",
    "            if subset.duplicated().sum() == 0:\n",
    "                composite_keys.append(cols)\n",
    "    return composite_keys\n",
    "\n",
    "def select_best_composite_key(composite_keys, data):\n",
    "    composite_keys = sorted(composite_keys, key=len)\n",
    "    \n",
    "    best_key = None\n",
    "    max_uniqueness = -1\n",
    "    for key in composite_keys:\n",
    "        subset = data[list(key)].dropna()\n",
    "        uniqueness_ratio = subset.drop_duplicates().shape[0] / data.shape[0]\n",
    "        \n",
    "        if uniqueness_ratio > max_uniqueness:\n",
    "            max_uniqueness = uniqueness_ratio\n",
    "            best_key = key\n",
    "    \n",
    "    return best_key\n",
    "\n",
    "def find_duplicate_rows(data, max_columns=3):\n",
    "    duplicate_rows = pd.DataFrame()\n",
    "    for r in range(2, max_columns+1):\n",
    "        for cols in combinations(data.columns, r):\n",
    "            subset = data[list(cols)].dropna()\n",
    "            duplicates = subset[subset.duplicated(keep=False)]\n",
    "            if not duplicates.empty:\n",
    "                duplicate_rows = pd.concat([duplicate_rows, duplicates])\n",
    "    return duplicate_rows.drop_duplicates()\n",
    "\n",
    "primary_key_candidates = check_unique_columns(data)\n",
    "\n",
    "if not primary_key_candidates:\n",
    "    composite_key_candidates = check_composite_keys(data)\n",
    "    print(composite_key_candidates)\n",
    "    best_composite_key = select_best_composite_key(composite_key_candidates, data)\n",
    "    print(f\"Best composite key: {best_composite_key if best_composite_key else 'None found'}\")\n",
    "\n",
    "# if best_composite_key:\n",
    "#     with open(\"/mnt/data/best_composite_key.txt\", \"w\") as f:\n",
    "#         f.write(f\"Best composite key: {best_composite_key}\\n\")\n",
    "# else:\n",
    "#     with open(\"/mnt/data/best_composite_key.txt\", \"w\") as f:\n",
    "#         f.write(\"No suitable composite key found\\n\")\n",
    "\n",
    "print(\"Potential primary key(s):\", primary_key_candidates if primary_key_candidates else \"None found\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from itertools import combinations\n",
    "\n",
    "file_path = 'C:/Users/rr010/OneDrive/Desktop/Class/Projects/TruckDelay_Classification/Data/Training_Data/traffic_table.csv'\n",
    "\n",
    "data = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicated rows for columns ('route_id', 'date', 'hour'):\n",
      "           route_id        date  hour\n",
      "1483807  R-4d24f472  2019-01-02   700\n",
      "1483808  R-4d24f472  2019-01-02   700\n",
      "1483844  R-4d24f472  2019-01-03  1900\n",
      "1483845  R-4d24f472  2019-01-03  1900\n",
      "1483934  R-4d24f472  2019-01-07  1200\n",
      "...             ...         ...   ...\n",
      "1537999  R-af1c0f31  2019-02-13     0\n",
      "1538016  R-af1c0f31  2019-02-13  1700\n",
      "1538017  R-af1c0f31  2019-02-13  1700\n",
      "1538061  R-af1c0f31  2019-02-15  1300\n",
      "1538062  R-af1c0f31  2019-02-15  1300\n",
      "\n",
      "[2610 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "max_columns = 3\n",
    "composite_keys = []\n",
    "for r in range(2, max_columns+1):\n",
    "    for cols in combinations(data.columns, r):\n",
    "        subset = data[list(cols)].dropna()\n",
    "\n",
    "        if cols == ('route_id', 'date', 'hour'):\n",
    "            duplicated_rows = subset[subset.duplicated(keep=False)]\n",
    "            print(f\"Duplicated rows for columns {cols}:\")\n",
    "            print(duplicated_rows)\n",
    "            \n",
    "        if subset.duplicated().sum() == 0:\n",
    "            composite_keys.append(cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected. Call `.close()` to terminate connection gracefully.\n",
      "\n",
      "Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/1022104\n",
      "Connected. Call `.close()` to terminate connection gracefully.\n"
     ]
    }
   ],
   "source": [
    "import hopsworks\n",
    "\n",
    "project = hopsworks.login(api_key_value = 'Io64c8Gc2JVuT4Oq.z8aTzYUIfvuktROte0AVSjoUK8O4MmmkrM9Oh5ZEsIoUUR3vI4iRmeztj3pi2jII')\n",
    "\n",
    "fs = project.get_feature_store()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine, MetaData\n",
    "\n",
    "def convert_tables_to_dataframes(engine):\n",
    "    metadata = MetaData()\n",
    "    metadata.reflect(bind=engine)\n",
    "    \n",
    "    df_dict = {}\n",
    "    \n",
    "    for table_name, table in metadata.tables.items():\n",
    "        with engine.connect() as conn:\n",
    "            df = pd.read_sql_table(table_name, conn)\n",
    "            df_dict[table_name] = df\n",
    "            print(f\"Table '{table_name}' converted to DataFrame.\")\n",
    "    \n",
    "    return df_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded DataFrame dictionary from \n",
      "'id' column added and set as index for table: city_weather\n",
      "'id' column added and set as index for table: routes_weather\n",
      "'id' column added and set as index for table: trucks_table\n",
      "'id' column added and set as index for table: drivers_table\n",
      "'id' column added and set as index for table: truck_schedule_table\n",
      "'id' column added and set as index for table: routes_table\n",
      "'id' column added and set as index for table: traffic_table\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>city_id</th>\n",
       "      <th>date</th>\n",
       "      <th>hour</th>\n",
       "      <th>temp</th>\n",
       "      <th>wind_speed</th>\n",
       "      <th>description</th>\n",
       "      <th>precip</th>\n",
       "      <th>humidity</th>\n",
       "      <th>visibility</th>\n",
       "      <th>pressure</th>\n",
       "      <th>chanceofrain</th>\n",
       "      <th>chanceoffog</th>\n",
       "      <th>chanceofsnow</th>\n",
       "      <th>chanceofthunder</th>\n",
       "      <th>event_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>C-927ceb5e</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>11</td>\n",
       "      <td>Light snow</td>\n",
       "      <td>0.0</td>\n",
       "      <td>86</td>\n",
       "      <td>6</td>\n",
       "      <td>1019</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2024-09-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>C-927ceb5e</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>100</td>\n",
       "      <td>28</td>\n",
       "      <td>12</td>\n",
       "      <td>Light snow</td>\n",
       "      <td>0.0</td>\n",
       "      <td>86</td>\n",
       "      <td>5</td>\n",
       "      <td>1021</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2024-09-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>C-927ceb5e</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>200</td>\n",
       "      <td>28</td>\n",
       "      <td>13</td>\n",
       "      <td>Moderate snow</td>\n",
       "      <td>0.0</td>\n",
       "      <td>85</td>\n",
       "      <td>4</td>\n",
       "      <td>1022</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2024-09-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>C-927ceb5e</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>300</td>\n",
       "      <td>28</td>\n",
       "      <td>14</td>\n",
       "      <td>Moderate snow</td>\n",
       "      <td>0.0</td>\n",
       "      <td>84</td>\n",
       "      <td>3</td>\n",
       "      <td>1024</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2024-09-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>C-927ceb5e</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>400</td>\n",
       "      <td>28</td>\n",
       "      <td>13</td>\n",
       "      <td>Moderate snow</td>\n",
       "      <td>0.0</td>\n",
       "      <td>84</td>\n",
       "      <td>3</td>\n",
       "      <td>1025</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2024-09-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55171</th>\n",
       "      <td>55172</td>\n",
       "      <td>C-594514f8</td>\n",
       "      <td>2019-02-15</td>\n",
       "      <td>1900</td>\n",
       "      <td>23</td>\n",
       "      <td>3</td>\n",
       "      <td>Cloudy</td>\n",
       "      <td>0.0</td>\n",
       "      <td>74</td>\n",
       "      <td>6</td>\n",
       "      <td>1018</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2024-09-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55172</th>\n",
       "      <td>55173</td>\n",
       "      <td>C-594514f8</td>\n",
       "      <td>2019-02-15</td>\n",
       "      <td>2000</td>\n",
       "      <td>23</td>\n",
       "      <td>3</td>\n",
       "      <td>Light snow</td>\n",
       "      <td>0.0</td>\n",
       "      <td>74</td>\n",
       "      <td>6</td>\n",
       "      <td>1018</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2024-09-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55173</th>\n",
       "      <td>55174</td>\n",
       "      <td>C-594514f8</td>\n",
       "      <td>2019-02-15</td>\n",
       "      <td>2100</td>\n",
       "      <td>23</td>\n",
       "      <td>3</td>\n",
       "      <td>Light snow</td>\n",
       "      <td>0.0</td>\n",
       "      <td>75</td>\n",
       "      <td>6</td>\n",
       "      <td>1019</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2024-09-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55174</th>\n",
       "      <td>55175</td>\n",
       "      <td>C-594514f8</td>\n",
       "      <td>2019-02-15</td>\n",
       "      <td>2200</td>\n",
       "      <td>23</td>\n",
       "      <td>2</td>\n",
       "      <td>Light snow</td>\n",
       "      <td>0.0</td>\n",
       "      <td>75</td>\n",
       "      <td>6</td>\n",
       "      <td>1019</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2024-09-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55175</th>\n",
       "      <td>55176</td>\n",
       "      <td>C-594514f8</td>\n",
       "      <td>2019-02-15</td>\n",
       "      <td>2300</td>\n",
       "      <td>23</td>\n",
       "      <td>2</td>\n",
       "      <td>Light snow</td>\n",
       "      <td>0.0</td>\n",
       "      <td>75</td>\n",
       "      <td>6</td>\n",
       "      <td>1019</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2024-09-19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>55176 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id     city_id        date  hour  temp  wind_speed    description  \\\n",
       "0          1  C-927ceb5e  2019-01-01     0    30          11     Light snow   \n",
       "1          2  C-927ceb5e  2019-01-01   100    28          12     Light snow   \n",
       "2          3  C-927ceb5e  2019-01-01   200    28          13  Moderate snow   \n",
       "3          4  C-927ceb5e  2019-01-01   300    28          14  Moderate snow   \n",
       "4          5  C-927ceb5e  2019-01-01   400    28          13  Moderate snow   \n",
       "...      ...         ...         ...   ...   ...         ...            ...   \n",
       "55171  55172  C-594514f8  2019-02-15  1900    23           3         Cloudy   \n",
       "55172  55173  C-594514f8  2019-02-15  2000    23           3     Light snow   \n",
       "55173  55174  C-594514f8  2019-02-15  2100    23           3     Light snow   \n",
       "55174  55175  C-594514f8  2019-02-15  2200    23           2     Light snow   \n",
       "55175  55176  C-594514f8  2019-02-15  2300    23           2     Light snow   \n",
       "\n",
       "       precip  humidity  visibility  pressure  chanceofrain  chanceoffog  \\\n",
       "0         0.0        86           6      1019             0            0   \n",
       "1         0.0        86           5      1021             0            0   \n",
       "2         0.0        85           4      1022             0            0   \n",
       "3         0.0        84           3      1024             0            0   \n",
       "4         0.0        84           3      1025             0            0   \n",
       "...       ...       ...         ...       ...           ...          ...   \n",
       "55171     0.0        74           6      1018             0            0   \n",
       "55172     0.0        74           6      1018             0            0   \n",
       "55173     0.0        75           6      1019             0            0   \n",
       "55174     0.0        75           6      1019             0            0   \n",
       "55175     0.0        75           6      1019             0            0   \n",
       "\n",
       "       chanceofsnow  chanceofthunder  event_time  \n",
       "0                 0                0  2024-09-19  \n",
       "1                 0                0  2024-09-19  \n",
       "2                 0                0  2024-09-19  \n",
       "3                 0                0  2024-09-19  \n",
       "4                 0                0  2024-09-19  \n",
       "...             ...              ...         ...  \n",
       "55171             0                0  2024-09-19  \n",
       "55172             0                0  2024-09-19  \n",
       "55173             0                0  2024-09-19  \n",
       "55174             0                0  2024-09-19  \n",
       "55175             0                0  2024-09-19  \n",
       "\n",
       "[55176 rows x 16 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import joblib\n",
    "from datetime import datetime\n",
    "from sqlalchemy import create_engine, MetaData\n",
    "\n",
    "def add_id_column_to_dataframes(df_dict):\n",
    "    for table_name, df in df_dict.items():\n",
    "        df.insert(0, 'id', range(1, len(df) + 1))  \n",
    "        \n",
    "        \n",
    "        today_date = datetime.now().date()  \n",
    "        df['event_time'] = [today_date] * len(df)\n",
    "        \n",
    "        df_dict[table_name] = df\n",
    "        \n",
    "        print(f\"'id' column added and set as index for table: {table_name}\")\n",
    "        \n",
    "        df_dict[table_name]\n",
    "\n",
    "    return df_dict\n",
    "\n",
    "DATABASE_URI = 'postgresql://postgres:rahul@localhost:5432/truck_delay'\n",
    "\n",
    "engine = create_engine(DATABASE_URI)\n",
    "\n",
    "dataframes_dict = convert_tables_to_dataframes(engine)\n",
    "print(f\"Loaded DataFrame dictionary from \")\n",
    "\n",
    "updated_dataframes_dict = add_id_column_to_dataframes(dataframes_dict)\n",
    "\n",
    "updated_dataframes_dict['city_weather']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for table_name, df in updated_dataframes_dict.items():\n",
    "#         # Identify string/object columns\n",
    "#         string_columns = df.select_dtypes(include=['object']).columns\n",
    "        \n",
    "        # for col in string_columns:\n",
    "        #     if False:\n",
    "        #         # Drop rows where there are missing values in the string columns\n",
    "        #         df.dropna(subset=[col], inplace=True)\n",
    "        #     else:\n",
    "        #         # Fill missing values in string columns with the specified fill_value\n",
    "        #         df[col].fillna(fill_value, inplace=True)\n",
    "        \n",
    "        # Update the DataFrame in the dictionary\n",
    "        # updated_dataframes_dict[table_name] = df\n",
    "        \n",
    "        # print(f\"String columns cleaned for table: {table_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table: trucks_table\n",
      "fuel_type    40\n",
      "dtype: int64\n",
      "Table: drivers_table\n",
      "gender           23\n",
      "driving_style    52\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "for table_name, df in updated_dataframes_dict.items():\n",
    "    string_columns = df.select_dtypes(include=['object'])\n",
    "    missing_values = string_columns.isna().sum()\n",
    "\n",
    "    missing_columns = missing_values[missing_values > 0]\n",
    "\n",
    "    if not missing_columns.empty:\n",
    "        print(f\"Table: {table_name}\")\n",
    "        print(missing_columns)\n",
    "        \n",
    "        for col in missing_columns.index:\n",
    "            mode_value = df[col].mode()[0]  \n",
    "            df[col].fillna(mode_value, inplace=True)\n",
    "        \n",
    "        updated_dataframes_dict[table_name] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                       0\n",
       "truck_id                 0\n",
       "truck_age                0\n",
       "load_capacity_pounds    57\n",
       "mileage_mpg              0\n",
       "fuel_type                0\n",
       "event_time               0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updated_dataframes_dict['trucks_table'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-09-19 14:42:32,878 WARNING: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "\n",
      "Column 'city_id' in table 'city_weather' is not a valid date format.\n",
      "Converted 'date' to datetime in table 'city_weather'\n",
      "2024-09-19 14:42:32,994 WARNING: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "\n",
      "Column 'description' in table 'city_weather' is not a valid date format.\n",
      "Converted 'event_time' to datetime in table 'city_weather'\n",
      "2024-09-19 14:42:33,213 WARNING: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "\n",
      "Column 'route_id' in table 'routes_weather' is not a valid date format.\n",
      "Converted 'Date' to datetime in table 'routes_weather'\n",
      "2024-09-19 14:42:33,380 WARNING: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "\n",
      "Column 'description' in table 'routes_weather' is not a valid date format.\n",
      "Converted 'event_time' to datetime in table 'routes_weather'\n",
      "2024-09-19 14:42:33,425 WARNING: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "\n",
      "Column 'fuel_type' in table 'trucks_table' is not a valid date format.\n",
      "Converted 'event_time' to datetime in table 'trucks_table'\n",
      "2024-09-19 14:42:33,433 WARNING: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "\n",
      "Column 'driver_id' in table 'drivers_table' is not a valid date format.\n",
      "2024-09-19 14:42:33,437 WARNING: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "\n",
      "Column 'name' in table 'drivers_table' is not a valid date format.\n",
      "2024-09-19 14:42:33,445 WARNING: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "\n",
      "Column 'gender' in table 'drivers_table' is not a valid date format.\n",
      "2024-09-19 14:42:33,451 WARNING: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "\n",
      "Column 'driving_style' in table 'drivers_table' is not a valid date format.\n",
      "Converted 'event_time' to datetime in table 'drivers_table'\n",
      "2024-09-19 14:42:33,465 WARNING: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "\n",
      "Column 'route_id' in table 'truck_schedule_table' is not a valid date format.\n",
      "Converted 'departure_date' to datetime in table 'truck_schedule_table'\n",
      "Converted 'estimated_arrival' to datetime in table 'truck_schedule_table'\n",
      "Converted 'event_time' to datetime in table 'truck_schedule_table'\n",
      "2024-09-19 14:42:33,506 WARNING: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "\n",
      "Column 'route_id' in table 'routes_table' is not a valid date format.\n",
      "2024-09-19 14:42:33,510 WARNING: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "\n",
      "Column 'origin_id' in table 'routes_table' is not a valid date format.\n",
      "2024-09-19 14:42:33,514 WARNING: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "\n",
      "Column 'destination_id' in table 'routes_table' is not a valid date format.\n",
      "Converted 'event_time' to datetime in table 'routes_table'\n",
      "2024-09-19 14:42:34,521 WARNING: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "\n",
      "Column 'route_id' in table 'traffic_table' is not a valid date format.\n",
      "Converted 'date' to datetime in table 'traffic_table'\n",
      "Converted 'event_time' to datetime in table 'traffic_table'\n"
     ]
    }
   ],
   "source": [
    "for table_name, df in updated_dataframes_dict.items():\n",
    "        \n",
    "        object_columns = df.select_dtypes(include=['object']).columns\n",
    "        \n",
    "        for col in object_columns:\n",
    "            try:\n",
    "                df[col] = pd.to_datetime(df[col], errors='raise')\n",
    "                print(f\"Converted '{col}' to datetime in table '{table_name}'\")\n",
    "            except (ValueError, TypeError):\n",
    "                print(f\"Column '{col}' in table '{table_name}' is not a valid date format.\")\n",
    "        \n",
    "        updated_dataframes_dict[table_name] = df\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                          int64\n",
       "city_id                    object\n",
       "date               datetime64[ns]\n",
       "hour                        int64\n",
       "temp                        int64\n",
       "wind_speed                  int64\n",
       "description                object\n",
       "precip                    float64\n",
       "humidity                    int64\n",
       "visibility                  int64\n",
       "pressure                    int64\n",
       "chanceofrain                int64\n",
       "chanceoffog                 int64\n",
       "chanceofsnow                int64\n",
       "chanceofthunder             int64\n",
       "event_time         datetime64[ns]\n",
       "dtype: object"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updated_dataframes_dict['city_weather'].dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>city_id</th>\n",
       "      <th>date</th>\n",
       "      <th>hour</th>\n",
       "      <th>temp</th>\n",
       "      <th>wind_speed</th>\n",
       "      <th>description</th>\n",
       "      <th>precip</th>\n",
       "      <th>humidity</th>\n",
       "      <th>visibility</th>\n",
       "      <th>pressure</th>\n",
       "      <th>chanceofrain</th>\n",
       "      <th>chanceoffog</th>\n",
       "      <th>chanceofsnow</th>\n",
       "      <th>chanceofthunder</th>\n",
       "      <th>event_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>C-927ceb5e</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>11</td>\n",
       "      <td>Light snow</td>\n",
       "      <td>0.0</td>\n",
       "      <td>86</td>\n",
       "      <td>6</td>\n",
       "      <td>1019</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2024-09-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>C-927ceb5e</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>100</td>\n",
       "      <td>28</td>\n",
       "      <td>12</td>\n",
       "      <td>Light snow</td>\n",
       "      <td>0.0</td>\n",
       "      <td>86</td>\n",
       "      <td>5</td>\n",
       "      <td>1021</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2024-09-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>C-927ceb5e</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>200</td>\n",
       "      <td>28</td>\n",
       "      <td>13</td>\n",
       "      <td>Moderate snow</td>\n",
       "      <td>0.0</td>\n",
       "      <td>85</td>\n",
       "      <td>4</td>\n",
       "      <td>1022</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2024-09-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>C-927ceb5e</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>300</td>\n",
       "      <td>28</td>\n",
       "      <td>14</td>\n",
       "      <td>Moderate snow</td>\n",
       "      <td>0.0</td>\n",
       "      <td>84</td>\n",
       "      <td>3</td>\n",
       "      <td>1024</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2024-09-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>C-927ceb5e</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>400</td>\n",
       "      <td>28</td>\n",
       "      <td>13</td>\n",
       "      <td>Moderate snow</td>\n",
       "      <td>0.0</td>\n",
       "      <td>84</td>\n",
       "      <td>3</td>\n",
       "      <td>1025</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2024-09-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55171</th>\n",
       "      <td>55172</td>\n",
       "      <td>C-594514f8</td>\n",
       "      <td>2019-02-15</td>\n",
       "      <td>1900</td>\n",
       "      <td>23</td>\n",
       "      <td>3</td>\n",
       "      <td>Cloudy</td>\n",
       "      <td>0.0</td>\n",
       "      <td>74</td>\n",
       "      <td>6</td>\n",
       "      <td>1018</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2024-09-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55172</th>\n",
       "      <td>55173</td>\n",
       "      <td>C-594514f8</td>\n",
       "      <td>2019-02-15</td>\n",
       "      <td>2000</td>\n",
       "      <td>23</td>\n",
       "      <td>3</td>\n",
       "      <td>Light snow</td>\n",
       "      <td>0.0</td>\n",
       "      <td>74</td>\n",
       "      <td>6</td>\n",
       "      <td>1018</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2024-09-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55173</th>\n",
       "      <td>55174</td>\n",
       "      <td>C-594514f8</td>\n",
       "      <td>2019-02-15</td>\n",
       "      <td>2100</td>\n",
       "      <td>23</td>\n",
       "      <td>3</td>\n",
       "      <td>Light snow</td>\n",
       "      <td>0.0</td>\n",
       "      <td>75</td>\n",
       "      <td>6</td>\n",
       "      <td>1019</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2024-09-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55174</th>\n",
       "      <td>55175</td>\n",
       "      <td>C-594514f8</td>\n",
       "      <td>2019-02-15</td>\n",
       "      <td>2200</td>\n",
       "      <td>23</td>\n",
       "      <td>2</td>\n",
       "      <td>Light snow</td>\n",
       "      <td>0.0</td>\n",
       "      <td>75</td>\n",
       "      <td>6</td>\n",
       "      <td>1019</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2024-09-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55175</th>\n",
       "      <td>55176</td>\n",
       "      <td>C-594514f8</td>\n",
       "      <td>2019-02-15</td>\n",
       "      <td>2300</td>\n",
       "      <td>23</td>\n",
       "      <td>2</td>\n",
       "      <td>Light snow</td>\n",
       "      <td>0.0</td>\n",
       "      <td>75</td>\n",
       "      <td>6</td>\n",
       "      <td>1019</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2024-09-19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>55176 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id     city_id       date  hour  temp  wind_speed    description  \\\n",
       "0          1  C-927ceb5e 2019-01-01     0    30          11     Light snow   \n",
       "1          2  C-927ceb5e 2019-01-01   100    28          12     Light snow   \n",
       "2          3  C-927ceb5e 2019-01-01   200    28          13  Moderate snow   \n",
       "3          4  C-927ceb5e 2019-01-01   300    28          14  Moderate snow   \n",
       "4          5  C-927ceb5e 2019-01-01   400    28          13  Moderate snow   \n",
       "...      ...         ...        ...   ...   ...         ...            ...   \n",
       "55171  55172  C-594514f8 2019-02-15  1900    23           3         Cloudy   \n",
       "55172  55173  C-594514f8 2019-02-15  2000    23           3     Light snow   \n",
       "55173  55174  C-594514f8 2019-02-15  2100    23           3     Light snow   \n",
       "55174  55175  C-594514f8 2019-02-15  2200    23           2     Light snow   \n",
       "55175  55176  C-594514f8 2019-02-15  2300    23           2     Light snow   \n",
       "\n",
       "       precip  humidity  visibility  pressure  chanceofrain  chanceoffog  \\\n",
       "0         0.0        86           6      1019             0            0   \n",
       "1         0.0        86           5      1021             0            0   \n",
       "2         0.0        85           4      1022             0            0   \n",
       "3         0.0        84           3      1024             0            0   \n",
       "4         0.0        84           3      1025             0            0   \n",
       "...       ...       ...         ...       ...           ...          ...   \n",
       "55171     0.0        74           6      1018             0            0   \n",
       "55172     0.0        74           6      1018             0            0   \n",
       "55173     0.0        75           6      1019             0            0   \n",
       "55174     0.0        75           6      1019             0            0   \n",
       "55175     0.0        75           6      1019             0            0   \n",
       "\n",
       "       chanceofsnow  chanceofthunder event_time  \n",
       "0                 0                0 2024-09-19  \n",
       "1                 0                0 2024-09-19  \n",
       "2                 0                0 2024-09-19  \n",
       "3                 0                0 2024-09-19  \n",
       "4                 0                0 2024-09-19  \n",
       "...             ...              ...        ...  \n",
       "55171             0                0 2024-09-19  \n",
       "55172             0                0 2024-09-19  \n",
       "55173             0                0 2024-09-19  \n",
       "55174             0                0 2024-09-19  \n",
       "55175             0                0 2024-09-19  \n",
       "\n",
       "[55176 rows x 16 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updated_dataframes_dict['city_weather']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_wetaher_fg = fs.get_or_create_feature_group(\n",
    "    name=\"city_wetaher_fg\",\n",
    "    version=1,\n",
    "    description=\"It is a table with a city weather at different times\",\n",
    "    primary_key=[\"id\"],\n",
    "    event_time=\"event_time\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Group created successfully, explore it at \n",
      "https://c.app.hopsworks.ai:443/p/1022104/fs/1013831/fg/1202527\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1529fd8fd944c50a8121dbd8a829172",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading Dataframe: 0.00% |          | Rows 0/55176 | Elapsed Time: 00:00 | Remaining Time: ?"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching job: city_wetaher_fg_1_offline_fg_materialization\n",
      "Job started successfully, you can follow the progress at \n",
      "https://c.app.hopsworks.ai/p/1022104/jobs/named/city_wetaher_fg_1_offline_fg_materialization/executions\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<hsfs.core.job.Job at 0x1c0446eb6d0>, None)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "city_wetaher_fg.insert(updated_dataframes_dict['city_weather'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing feature group for table: city_weather (Feature Group: city_weather_fg)\n",
      "Feature Group created successfully, explore it at \n",
      "https://c.app.hopsworks.ai:443/p/1022104/fs/1013831/fg/1202528\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5ca46f9f52e4dd7bb375d00ec064075",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading Dataframe: 0.00% |          | Rows 0/55176 | Elapsed Time: 00:00 | Remaining Time: ?"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching job: city_weather_fg_1_offline_fg_materialization\n",
      "Job started successfully, you can follow the progress at \n",
      "https://c.app.hopsworks.ai/p/1022104/jobs/named/city_weather_fg_1_offline_fg_materialization/executions\n",
      "Inserted data into feature group: city_weather_fg\n",
      "Processing feature group for table: routes_weather (Feature Group: routes_weather_fg)\n",
      "2024-09-19 15:04:33,174 WARNING: FeatureGroupWarning: The ingested dataframe contains upper case letters in feature names: `['Date']`. Feature names are sanitized to lower case in the feature store.\n",
      "\n",
      "Feature Group created successfully, explore it at \n",
      "https://c.app.hopsworks.ai:443/p/1022104/fs/1013831/fg/1204568\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40d419bb36304200b353d52cbf330eff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading Dataframe: 0.00% |          | Rows 0/425712 | Elapsed Time: 00:00 | Remaining Time: ?"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching job: routes_weather_fg_1_offline_fg_materialization\n",
      "Job started successfully, you can follow the progress at \n",
      "https://c.app.hopsworks.ai/p/1022104/jobs/named/routes_weather_fg_1_offline_fg_materialization/executions\n",
      "Inserted data into feature group: routes_weather_fg\n",
      "Processing feature group for table: trucks_table (Feature Group: trucks_table_fg)\n",
      "Feature Group created successfully, explore it at \n",
      "https://c.app.hopsworks.ai:443/p/1022104/fs/1013831/fg/1202529\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5ca6f39109348ea9f36375e69f14321",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading Dataframe: 0.00% |          | Rows 0/1300 | Elapsed Time: 00:00 | Remaining Time: ?"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching job: trucks_table_fg_1_offline_fg_materialization\n",
      "Job started successfully, you can follow the progress at \n",
      "https://c.app.hopsworks.ai/p/1022104/jobs/named/trucks_table_fg_1_offline_fg_materialization/executions\n",
      "Inserted data into feature group: trucks_table_fg\n",
      "Processing feature group for table: drivers_table (Feature Group: drivers_table_fg)\n",
      "Feature Group created successfully, explore it at \n",
      "https://c.app.hopsworks.ai:443/p/1022104/fs/1013831/fg/1203546\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8703294a1276414fb45d2e301b94c1d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading Dataframe: 0.00% |          | Rows 0/1300 | Elapsed Time: 00:00 | Remaining Time: ?"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching job: drivers_table_fg_1_offline_fg_materialization\n",
      "Job started successfully, you can follow the progress at \n",
      "https://c.app.hopsworks.ai/p/1022104/jobs/named/drivers_table_fg_1_offline_fg_materialization/executions\n",
      "Inserted data into feature group: drivers_table_fg\n",
      "Processing feature group for table: truck_schedule_table (Feature Group: truck_schedule_table_fg)\n",
      "Feature Group created successfully, explore it at \n",
      "https://c.app.hopsworks.ai:443/p/1022104/fs/1013831/fg/1204569\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0a335e2939247838366ec288dc7082b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading Dataframe: 0.00% |          | Rows 0/12308 | Elapsed Time: 00:00 | Remaining Time: ?"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-09-19 15:05:07,513 WARNING: UserWarning: Discarding nonzero nanoseconds in conversion.\n",
      "\n",
      "Launching job: truck_schedule_table_fg_1_offline_fg_materialization\n",
      "Job started successfully, you can follow the progress at \n",
      "https://c.app.hopsworks.ai/p/1022104/jobs/named/truck_schedule_table_fg_1_offline_fg_materialization/executions\n",
      "Inserted data into feature group: truck_schedule_table_fg\n",
      "Processing feature group for table: routes_table (Feature Group: routes_table_fg)\n",
      "Feature Group created successfully, explore it at \n",
      "https://c.app.hopsworks.ai:443/p/1022104/fs/1013831/fg/1204570\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bd73b83c8bf4833a35776cffaf3e3bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading Dataframe: 0.00% |          | Rows 0/2352 | Elapsed Time: 00:00 | Remaining Time: ?"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching job: routes_table_fg_1_offline_fg_materialization\n"
     ]
    },
    {
     "ename": "RestAPIError",
     "evalue": "Metadata operation error: (url: https://c.app.hopsworks.ai/hopsworks-api/api/project/1022104/jobs/routes_table_fg_1_offline_fg_materialization/executions). Server response: \nHTTP code: 400, HTTP reason: Bad Request, body: b'{\"errorCode\":130040,\"usrMsg\":\"Parallel executions quota reached for Project: Truck_Delay2 Current 5 Max: 5\",\"devMsg\":\"Parallel executions quota reached for Project: Truck_Delay2 Current 5 Max: 5\",\"errorMsg\":\"Job reached the maximum number of executions.\"}', error code: 130040, error msg: Job reached the maximum number of executions., user msg: Parallel executions quota reached for Project: Truck_Delay2 Current 5 Max: 5",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRestAPIError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[48], line 20\u001b[0m\n\u001b[0;32m     11\u001b[0m feature_group \u001b[38;5;241m=\u001b[39m fs\u001b[38;5;241m.\u001b[39mget_or_create_feature_group(\n\u001b[0;32m     12\u001b[0m     name\u001b[38;5;241m=\u001b[39mfeature_group_name,  \u001b[38;5;66;03m# Use 'tablename_fg' as the feature group name\u001b[39;00m\n\u001b[0;32m     13\u001b[0m     version\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     16\u001b[0m     event_time\u001b[38;5;241m=\u001b[39mevent_time_column\n\u001b[0;32m     17\u001b[0m )\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Insert the DataFrame into the feature group\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m \u001b[43mfeature_group\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minsert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInserted data into feature group: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfeature_group_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\rr010\\OneDrive\\Desktop\\Class\\Projects\\TruckDelay_Classification\\.venv\\lib\\site-packages\\hsfs\\feature_group.py:2528\u001b[0m, in \u001b[0;36mFeatureGroup.insert\u001b[1;34m(self, features, overwrite, operation, storage, write_options, validation_options, save_code, wait)\u001b[0m\n\u001b[0;32m   2525\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwait_for_job\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m write_options:\n\u001b[0;32m   2526\u001b[0m     write_options[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwait_for_job\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m wait\n\u001b[1;32m-> 2528\u001b[0m job, ge_report \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_feature_group_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minsert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2529\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2530\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeature_dataframe\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_dataframe\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2531\u001b[0m \u001b[43m    \u001b[49m\u001b[43moverwrite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2532\u001b[0m \u001b[43m    \u001b[49m\u001b[43moperation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moperation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2533\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlower\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   2534\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwrite_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwrite_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2535\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msave_report\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mvalidation_options\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2536\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2537\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m save_code \u001b[38;5;129;01mand\u001b[39;00m (\n\u001b[0;32m   2538\u001b[0m     ge_report \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m ge_report\u001b[38;5;241m.\u001b[39mingestion_result \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mINGESTED\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2539\u001b[0m ):\n\u001b[0;32m   2540\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_code_engine\u001b[38;5;241m.\u001b[39msave_code(\u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\rr010\\OneDrive\\Desktop\\Class\\Projects\\TruckDelay_Classification\\.venv\\lib\\site-packages\\hsfs\\core\\feature_group_engine.py:132\u001b[0m, in \u001b[0;36mFeatureGroupEngine.insert\u001b[1;34m(self, feature_group, feature_dataframe, overwrite, operation, storage, write_options, validation_options)\u001b[0m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m overwrite:\n\u001b[0;32m    129\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_feature_group_api\u001b[38;5;241m.\u001b[39mdelete_content(feature_group)\n\u001b[0;32m    131\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m--> 132\u001b[0m     \u001b[43mengine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_instance\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_dataframe\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    133\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeature_group\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    134\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeature_dataframe\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    135\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbulk_insert\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moverwrite\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moperation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    136\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeature_group\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43monline_enabled\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    137\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    138\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffline_write_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    139\u001b[0m \u001b[43m        \u001b[49m\u001b[43monline_write_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m    141\u001b[0m     ge_report,\n\u001b[0;32m    142\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\rr010\\OneDrive\\Desktop\\Class\\Projects\\TruckDelay_Classification\\.venv\\lib\\site-packages\\hsfs\\engine\\python.py:593\u001b[0m, in \u001b[0;36mEngine.save_dataframe\u001b[1;34m(self, feature_group, dataframe, operation, online_enabled, storage, offline_write_options, online_write_options, validation_id)\u001b[0m\n\u001b[0;32m    578\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msave_dataframe\u001b[39m(\n\u001b[0;32m    579\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    580\u001b[0m     feature_group: FeatureGroup,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    587\u001b[0m     validation_id: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    588\u001b[0m ):\n\u001b[0;32m    589\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    590\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(feature_group, ExternalFeatureGroup)\n\u001b[0;32m    591\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m feature_group\u001b[38;5;241m.\u001b[39monline_enabled\n\u001b[0;32m    592\u001b[0m     ) \u001b[38;5;129;01mor\u001b[39;00m feature_group\u001b[38;5;241m.\u001b[39mstream:\n\u001b[1;32m--> 593\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_write_dataframe_kafka\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    594\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfeature_group\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffline_write_options\u001b[49m\n\u001b[0;32m    595\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    596\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    597\u001b[0m         \u001b[38;5;66;03m# for backwards compatibility\u001b[39;00m\n\u001b[0;32m    598\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlegacy_save_dataframe(\n\u001b[0;32m    599\u001b[0m             feature_group,\n\u001b[0;32m    600\u001b[0m             dataframe,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    606\u001b[0m             validation_id,\n\u001b[0;32m    607\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\rr010\\OneDrive\\Desktop\\Class\\Projects\\TruckDelay_Classification\\.venv\\lib\\site-packages\\hsfs\\engine\\python.py:1106\u001b[0m, in \u001b[0;36mEngine._write_dataframe_kafka\u001b[1;34m(self, feature_group, dataframe, offline_write_options)\u001b[0m\n\u001b[0;32m   1104\u001b[0m         initial_check_point \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1105\u001b[0m     \u001b[38;5;66;03m# provide the initial_check_point as it will reduce the read amplification of materialization job\u001b[39;00m\n\u001b[1;32m-> 1106\u001b[0m     \u001b[43mfeature_group\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmaterialization_job\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1107\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_group\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmaterialization_job\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdefaultArgs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1108\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minitial_check_point\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1109\u001b[0m \u001b[43m        \u001b[49m\u001b[43mawait_termination\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffline_write_options\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mwait_for_job\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1110\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(feature_group, ExternalFeatureGroup):\n\u001b[0;32m   1112\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\rr010\\OneDrive\\Desktop\\Class\\Projects\\TruckDelay_Classification\\.venv\\lib\\site-packages\\hsfs\\core\\job.py:118\u001b[0m, in \u001b[0;36mJob.run\u001b[1;34m(self, args, await_termination)\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Run the job.\u001b[39;00m\n\u001b[0;32m     95\u001b[0m \n\u001b[0;32m     96\u001b[0m \u001b[38;5;124;03mRuns the job, by default awaiting its completion.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;124;03m    await_termination: Identifies if the client should wait for the job to complete, defaults to True.\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLaunching job: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 118\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_job_api\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlaunch\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    119\u001b[0m \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJob started successfully, you can follow the progress at \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    121\u001b[0m         util\u001b[38;5;241m.\u001b[39mget_job_url(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhref)\n\u001b[0;32m    122\u001b[0m     )\n\u001b[0;32m    123\u001b[0m )\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_for_job(await_termination\u001b[38;5;241m=\u001b[39mawait_termination)\n",
      "File \u001b[1;32mc:\\Users\\rr010\\OneDrive\\Desktop\\Class\\Projects\\TruckDelay_Classification\\.venv\\lib\\site-packages\\hsfs\\core\\job_api.py:39\u001b[0m, in \u001b[0;36mJobApi.launch\u001b[1;34m(self, name, args)\u001b[0m\n\u001b[0;32m     36\u001b[0m _client \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mget_instance()\n\u001b[0;32m     37\u001b[0m path_params \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mproject\u001b[39m\u001b[38;5;124m\"\u001b[39m, _client\u001b[38;5;241m.\u001b[39m_project_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjobs\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexecutions\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m---> 39\u001b[0m \u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_request\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPOST\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\rr010\\OneDrive\\Desktop\\Class\\Projects\\TruckDelay_Classification\\.venv\\lib\\site-packages\\hsfs\\decorators.py:35\u001b[0m, in \u001b[0;36mconnected.<locals>.if_connected\u001b[1;34m(inst, *args, **kwargs)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m inst\u001b[38;5;241m.\u001b[39m_connected:\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NoHopsworksConnectionError\n\u001b[1;32m---> 35\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fn(inst, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\rr010\\OneDrive\\Desktop\\Class\\Projects\\TruckDelay_Classification\\.venv\\lib\\site-packages\\hsfs\\client\\base.py:176\u001b[0m, in \u001b[0;36mClient._send_request\u001b[1;34m(self, method, path_params, query_params, headers, data, stream, files)\u001b[0m\n\u001b[0;32m    171\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retry_token_expired(\n\u001b[0;32m    172\u001b[0m         request, stream, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mTOKEN_EXPIRED_RETRY_INTERVAL, \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    173\u001b[0m     )\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m--> 176\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mRestAPIError(url, response)\n\u001b[0;32m    178\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stream:\n\u001b[0;32m    179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[1;31mRestAPIError\u001b[0m: Metadata operation error: (url: https://c.app.hopsworks.ai/hopsworks-api/api/project/1022104/jobs/routes_table_fg_1_offline_fg_materialization/executions). Server response: \nHTTP code: 400, HTTP reason: Bad Request, body: b'{\"errorCode\":130040,\"usrMsg\":\"Parallel executions quota reached for Project: Truck_Delay2 Current 5 Max: 5\",\"devMsg\":\"Parallel executions quota reached for Project: Truck_Delay2 Current 5 Max: 5\",\"errorMsg\":\"Job reached the maximum number of executions.\"}', error code: 130040, error msg: Job reached the maximum number of executions., user msg: Parallel executions quota reached for Project: Truck_Delay2 Current 5 Max: 5"
     ]
    }
   ],
   "source": [
    "for table_name, df in updated_dataframes_dict.items():\n",
    "    feature_group_name = f\"{table_name}_fg\"\n",
    "    \n",
    "    primary_key = ['id']\n",
    "    event_time_column = 'event_time'\n",
    "\n",
    "    print(f\"Processing feature group for table: {table_name} (Feature Group: {feature_group_name})\")\n",
    "    feature_group = fs.get_or_create_feature_group(\n",
    "        name=feature_group_name,  \n",
    "        version=1,\n",
    "        description=f\"Feature group for {feature_group_name}\",\n",
    "        primary_key=primary_key,\n",
    "        event_time=event_time_column\n",
    "    )\n",
    "    \n",
    "    feature_group.insert(df)\n",
    "    print(f\"Inserted data into feature group: {feature_group_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b40f33b20cc480e8953ffd2cfa040df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading Dataframe: 0.00% |          | Rows 0/2352 | Elapsed Time: 00:00 | Remaining Time: ?"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching job: routes_table_fg_1_offline_fg_materialization\n",
      "Job started successfully, you can follow the progress at \n",
      "https://c.app.hopsworks.ai/p/1022104/jobs/named/routes_table_fg_1_offline_fg_materialization/executions\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<hsfs.core.job.Job at 0x1c0446bd180>, None)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_group = fs.get_or_create_feature_group(\n",
    "    name=\"routes_table_fg\",\n",
    "    version=1,\n",
    "    description=\"Feature group for routes_table_fg\",\n",
    "    primary_key=[\"id\"],\n",
    "    event_time=\"event_time\",\n",
    ")\n",
    "\n",
    "feature_group.insert(updated_dataframes_dict['routes_table'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
